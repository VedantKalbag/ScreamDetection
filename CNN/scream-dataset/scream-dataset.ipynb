{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing existing numpy file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df=pd.DataFrame(np.load('/home/vedant/projects/ScreamDetection/resources/working_data/data_with_vggish.npy',allow_pickle=True),columns=['video_id','start_time','mid_ts','label','audio','vggish'])\n",
    " \n",
    "vgg=pd.DataFrame(df[\"vggish\"].to_list())\n",
    "cols=[]\n",
    "for col in vgg.columns:\n",
    "    cols.append(f\"vggish_{col+1}\")\n",
    "vgg.columns= cols\n",
    " \n",
    "df=pd.concat([df, vgg], axis=1)\n",
    "df=df.drop('vggish',axis=1)\n",
    " \n",
    "import soundfile as sf\n",
    "v=''\n",
    "i=0\n",
    "df.insert(0, \"block_number\", '')\n",
    "for index,row in df.iterrows():\n",
    "    #print(row)\n",
    "    video_id = row['video_id']\n",
    "    if video_id == v:\n",
    "        i+=1\n",
    "    else:\n",
    "        v=video_id\n",
    "        i=0\n",
    "    start_ts = row['start_time']\n",
    "    df.iloc[index,0] = i\n",
    "    out_name = f\"{i}_{video_id}_start{int(start_ts*1000)}ms\"\n",
    "    print(out_name)\n",
    " \n",
    "    audio = df.iloc[index,5]\n",
    "    sf.write(audio,\"/home/vedant/projects/ScreamDetection/resources/dataset/blocked_audio/\"+out_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('/home/vedant/projects/ScreamDetection/resources/dataset/dataset-pytorch.csv',header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['no_vocals', 'midfry', 'clean', 'highfry', 'lowfry', 'layered'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('/home/vedant/projects/ScreamDetection/resources/dataset/dataset-pytorch.csv',header=0)\n",
    "#df.insert(6,'label_mapped','')\n",
    "lst=[]\n",
    "for index,row in df.iterrows():\n",
    "    if row['label'] == 'no_vocals':\n",
    "        lst.append(0)\n",
    "    if row['label'] == 'midfry':\n",
    "        lst.append(1)\n",
    "    if row['label'] == 'clean':\n",
    "        lst.append(2)\n",
    "    if row['label'] == 'highfry':\n",
    "        lst.append(3)\n",
    "    if row['label'] == 'lowfry':\n",
    "        lst.append(4)\n",
    "    if row['label'] == 'layered':\n",
    "        lst.append(5)\n",
    "df.insert(6,'label_mapped',lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>block_number</th>\n",
       "      <th>video_id</th>\n",
       "      <th>start_time</th>\n",
       "      <th>mid_ts</th>\n",
       "      <th>label</th>\n",
       "      <th>filename</th>\n",
       "      <th>label_mapped</th>\n",
       "      <th>vggish_1</th>\n",
       "      <th>vggish_2</th>\n",
       "      <th>vggish_3</th>\n",
       "      <th>...</th>\n",
       "      <th>vggish_119</th>\n",
       "      <th>vggish_120</th>\n",
       "      <th>vggish_121</th>\n",
       "      <th>vggish_122</th>\n",
       "      <th>vggish_123</th>\n",
       "      <th>vggish_124</th>\n",
       "      <th>vggish_125</th>\n",
       "      <th>vggish_126</th>\n",
       "      <th>vggish_127</th>\n",
       "      <th>vggish_128</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>109</td>\n",
       "      <td>4600fGWcn9o</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>no_vocals</td>\n",
       "      <td>109_4600fGWcn9o_start0ms</td>\n",
       "      <td>0</td>\n",
       "      <td>185.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>127.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>255.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>110</td>\n",
       "      <td>4600fGWcn9o</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>no_vocals</td>\n",
       "      <td>110_4600fGWcn9o_start500ms</td>\n",
       "      <td>0</td>\n",
       "      <td>181.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>159.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>255.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>111</td>\n",
       "      <td>4600fGWcn9o</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>no_vocals</td>\n",
       "      <td>111_4600fGWcn9o_start1000ms</td>\n",
       "      <td>0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>178.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>210.0</td>\n",
       "      <td>175.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>189.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>255.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>112</td>\n",
       "      <td>4600fGWcn9o</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>no_vocals</td>\n",
       "      <td>112_4600fGWcn9o_start1500ms</td>\n",
       "      <td>0</td>\n",
       "      <td>183.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>181.0</td>\n",
       "      <td>188.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>209.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>203.0</td>\n",
       "      <td>235.0</td>\n",
       "      <td>255.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>113</td>\n",
       "      <td>4600fGWcn9o</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>no_vocals</td>\n",
       "      <td>113_4600fGWcn9o_start2000ms</td>\n",
       "      <td>0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>127.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>247.0</td>\n",
       "      <td>230.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>146.0</td>\n",
       "      <td>217.0</td>\n",
       "      <td>175.0</td>\n",
       "      <td>255.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33815</th>\n",
       "      <td>435</td>\n",
       "      <td>0m5fIHHfJTw</td>\n",
       "      <td>217.5</td>\n",
       "      <td>218.0</td>\n",
       "      <td>no_vocals</td>\n",
       "      <td>435_0m5fIHHfJTw_start217500ms</td>\n",
       "      <td>0</td>\n",
       "      <td>176.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>151.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>201.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>151.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>237.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>255.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33816</th>\n",
       "      <td>436</td>\n",
       "      <td>0m5fIHHfJTw</td>\n",
       "      <td>218.0</td>\n",
       "      <td>218.5</td>\n",
       "      <td>no_vocals</td>\n",
       "      <td>436_0m5fIHHfJTw_start218000ms</td>\n",
       "      <td>0</td>\n",
       "      <td>174.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>143.0</td>\n",
       "      <td>...</td>\n",
       "      <td>108.0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>197.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>255.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33817</th>\n",
       "      <td>437</td>\n",
       "      <td>0m5fIHHfJTw</td>\n",
       "      <td>218.5</td>\n",
       "      <td>219.0</td>\n",
       "      <td>no_vocals</td>\n",
       "      <td>437_0m5fIHHfJTw_start218500ms</td>\n",
       "      <td>0</td>\n",
       "      <td>171.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>...</td>\n",
       "      <td>20.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>255.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33818</th>\n",
       "      <td>438</td>\n",
       "      <td>0m5fIHHfJTw</td>\n",
       "      <td>219.0</td>\n",
       "      <td>219.5</td>\n",
       "      <td>no_vocals</td>\n",
       "      <td>438_0m5fIHHfJTw_start219000ms</td>\n",
       "      <td>0</td>\n",
       "      <td>174.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>138.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>138.0</td>\n",
       "      <td>136.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>139.0</td>\n",
       "      <td>233.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>255.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33819</th>\n",
       "      <td>439</td>\n",
       "      <td>0m5fIHHfJTw</td>\n",
       "      <td>219.5</td>\n",
       "      <td>220.0</td>\n",
       "      <td>no_vocals</td>\n",
       "      <td>439_0m5fIHHfJTw_start219500ms</td>\n",
       "      <td>0</td>\n",
       "      <td>176.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>154.0</td>\n",
       "      <td>159.0</td>\n",
       "      <td>183.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>207.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>255.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>33820 rows Ã— 135 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       block_number     video_id  start_time  mid_ts      label  \\\n",
       "0               109  4600fGWcn9o         0.0     0.5  no_vocals   \n",
       "1               110  4600fGWcn9o         0.5     1.0  no_vocals   \n",
       "2               111  4600fGWcn9o         1.0     1.5  no_vocals   \n",
       "3               112  4600fGWcn9o         1.5     2.0  no_vocals   \n",
       "4               113  4600fGWcn9o         2.0     2.5  no_vocals   \n",
       "...             ...          ...         ...     ...        ...   \n",
       "33815           435  0m5fIHHfJTw       217.5   218.0  no_vocals   \n",
       "33816           436  0m5fIHHfJTw       218.0   218.5  no_vocals   \n",
       "33817           437  0m5fIHHfJTw       218.5   219.0  no_vocals   \n",
       "33818           438  0m5fIHHfJTw       219.0   219.5  no_vocals   \n",
       "33819           439  0m5fIHHfJTw       219.5   220.0  no_vocals   \n",
       "\n",
       "                            filename  label_mapped  vggish_1  vggish_2  \\\n",
       "0           109_4600fGWcn9o_start0ms             0     185.0      16.0   \n",
       "1         110_4600fGWcn9o_start500ms             0     181.0      13.0   \n",
       "2        111_4600fGWcn9o_start1000ms             0     180.0      14.0   \n",
       "3        112_4600fGWcn9o_start1500ms             0     183.0      10.0   \n",
       "4        113_4600fGWcn9o_start2000ms             0     190.0      13.0   \n",
       "...                              ...           ...       ...       ...   \n",
       "33815  435_0m5fIHHfJTw_start217500ms             0     176.0      17.0   \n",
       "33816  436_0m5fIHHfJTw_start218000ms             0     174.0      15.0   \n",
       "33817  437_0m5fIHHfJTw_start218500ms             0     171.0      11.0   \n",
       "33818  438_0m5fIHHfJTw_start219000ms             0     174.0      12.0   \n",
       "33819  439_0m5fIHHfJTw_start219500ms             0     176.0      12.0   \n",
       "\n",
       "       vggish_3  ...  vggish_119  vggish_120  vggish_121  vggish_122  \\\n",
       "0         145.0  ...         0.0       228.0       127.0       255.0   \n",
       "1         148.0  ...         0.0       159.0        14.0       255.0   \n",
       "2         128.0  ...         0.0       178.0        59.0       255.0   \n",
       "3         118.0  ...         0.0       181.0       188.0       255.0   \n",
       "4         127.0  ...         0.0        97.0       247.0       230.0   \n",
       "...         ...  ...         ...         ...         ...         ...   \n",
       "33815     151.0  ...         0.0        59.0        90.0       201.0   \n",
       "33816     143.0  ...       108.0       195.0       197.0        60.0   \n",
       "33817     137.0  ...        20.0       165.0       121.0       128.0   \n",
       "33818     138.0  ...         0.0       138.0       136.0       186.0   \n",
       "33819     148.0  ...         3.0       154.0       159.0       183.0   \n",
       "\n",
       "       vggish_123  vggish_124  vggish_125  vggish_126  vggish_127  vggish_128  \n",
       "0            92.0       125.0        90.0        98.0         6.0       255.0  \n",
       "1            90.0       104.0       116.0        42.0        20.0       255.0  \n",
       "2           210.0       175.0        34.0       189.0        42.0       255.0  \n",
       "3           209.0        88.0       132.0       203.0       235.0       255.0  \n",
       "4           128.0        53.0       146.0       217.0       175.0       255.0  \n",
       "...           ...         ...         ...         ...         ...         ...  \n",
       "33815        65.0       151.0       119.0       237.0       121.0       255.0  \n",
       "33816       117.0        21.0       255.0       106.0         0.0       255.0  \n",
       "33817        58.0       121.0       255.0       130.0         0.0       255.0  \n",
       "33818        48.0       139.0       233.0        36.0         0.0       255.0  \n",
       "33819        83.0       111.0       207.0        45.0         0.0       255.0  \n",
       "\n",
       "[33820 rows x 135 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('/home/vedant/projects/ScreamDetection/resources/dataset/dataset-pytorch.csv',header=True, index=False,encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating ScreamDataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torchaudio\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UrbanSoundDataset(Dataset):\n",
    "    def __init__(self, annotation_file,audio_dir,transformation,\n",
    "                target_sample_rate,num_samples,device):\n",
    "        self.annotations = pd.read_csv(annotation_file)\n",
    "        self.audio_dir = audio_dir\n",
    "        self.device = device\n",
    "        self.transformation = transformation.to(self.device)\n",
    "        self.target_sample_rate = target_sample_rate\n",
    "        self.num_samples = num_samples\n",
    "\n",
    "    def __len__(self): #return number of elements in dataset\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def _get_audio_sample_path(self, index):\n",
    "        fold = f\"fold{self.annotations.iloc[index, 5]}\"\n",
    "        path = os.path.join(self.audio_dir,fold,self.annotations.iloc[index, 0 ])\n",
    "        return path\n",
    "    \n",
    "    def _get_audio_sample_label(self, index):\n",
    "        return self.annotations.iloc[index, 6]\n",
    "\n",
    "    def _resample_if_necessary(self,signal,sr):\n",
    "        if sr != self.target_sample_rate:\n",
    "            resampler = torchaudio.transforms.Resample(sr,self.target_sample_rate).to(self.device)\n",
    "            signal = resampler(signal)\n",
    "        return signal\n",
    "\n",
    "    def _mix_down_if_necessary(self,signal):\n",
    "        if signal.shape[0] > 1:\n",
    "            signal = torch.mean(signal,dim=0,keepdim=True)\n",
    "        return signal\n",
    "    \n",
    "    def _right_pad_if_necessary(self,signal):\n",
    "        length_of_signal = signal.shape[1]\n",
    "        if length_of_signal < self.num_samples:\n",
    "            num_missing_samples = self.num_samples - length_of_signal\n",
    "            last_dim_padding = (0,num_missing_samples) # (left_pad,right_pad)\n",
    "            signal = torch.nn.functional.pad(signal,last_dim_padding)\n",
    "        return signal\n",
    "\n",
    "    def _cut_if_necessary(self,signal):\n",
    "        if signal.shape[1] > self.num_samples:\n",
    "            signal = signal[:,:self.num_samples]\n",
    "        return signal\n",
    "\n",
    "    def __getitem__(self,index): #return item at an index\n",
    "        audio_sample_path = self._get_audio_sample_path(index)\n",
    "        label = self._get_audio_sample_label(index)\n",
    "        signal, sr = torchaudio.load(audio_sample_path)\n",
    "        signal = signal.to(self.device) #register signal to device\n",
    "        #Transformations\n",
    "        signal = self._resample_if_necessary(signal, sr) # convert to target sample rate\n",
    "        signal = self._mix_down_if_necessary(signal) # converting to mono\n",
    "        signal = self._right_pad_if_necessary(signal) # if num samples < target num samples\n",
    "        signal = self._cut_if_necessary(signal) # crop if num samples > target num samples\n",
    "        signal = self.transformation(signal) # Get mel spectrogram\n",
    "        return signal, label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScreamDataset(Dataset):\n",
    "    def __init__(self,annotation_file, audio_dir,transformation,device):\n",
    "        self.annotations = pd.read_csv(annotation_file,header=0)\n",
    "        self.audio_dir = audio_dir\n",
    "        self.device = device\n",
    "        self.transformation = transformation.to(self.device)\n",
    "\n",
    "    def _get_audio_sample_path(self, index):\n",
    "        filename = f\"{self.annotations.iloc[index, 5]}.wav\"\n",
    "        path = os.path.join(self.audio_dir,filename)\n",
    "        return path\n",
    "\n",
    "    def _get_audio_sample_label(self, index):\n",
    "        return self.annotations.iloc[index, 6]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    " \n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        audio_sample_path = self._get_audio_sample_path(index)\n",
    "        label = self._get_audio_sample_label(index)\n",
    "        signal, sr = torchaudio.load(audio_sample_path)\n",
    "        signal = signal.to(self.device) #register signal to device\n",
    "        #Transformations\n",
    "        signal = self.transformation(signal)\n",
    "        return signal, label\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_file= '/home/vedant/projects/ScreamDetection/resources/dataset/dataset-pytorch.csv'\n",
    "\n",
    "audio_dir= '/home/vedant/projects/ScreamDetection/resources/dataset/blocked_audio'\n",
    "\n",
    "mel_spectrogram = torchaudio.transforms.MelSpectrogram(\n",
    "        sample_rate = 44100,\n",
    "        n_fft = 1024,\n",
    "        hop_length = 512,\n",
    "        n_mels = 64\n",
    "    )\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = 'cuda'\n",
    "else:\n",
    "    DEVICE = 'cpu'\n",
    "\n",
    "\n",
    "sd = ScreamDataset(annotation_file, audio_dir, mel_spectrogram, DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.0000e+00, 0.0000e+00, 9.9935e-13,  ..., 4.5554e+02,\n",
       "           4.5855e+02, 7.8074e+02],\n",
       "          [0.0000e+00, 0.0000e+00, 1.0615e-12,  ..., 1.7162e+02,\n",
       "           4.5055e+02, 2.5639e+02],\n",
       "          [0.0000e+00, 0.0000e+00, 1.1111e-12,  ..., 2.7601e+01,\n",
       "           2.2501e+02, 3.6884e+03],\n",
       "          ...,\n",
       "          [0.0000e+00, 0.0000e+00, 1.7160e-12,  ..., 9.1529e-07,\n",
       "           6.5009e-06, 5.1676e-02],\n",
       "          [0.0000e+00, 0.0000e+00, 1.2231e-12,  ..., 1.2752e-06,\n",
       "           4.0309e-06, 5.2595e-02],\n",
       "          [0.0000e+00, 0.0000e+00, 1.2521e-12,  ..., 1.2896e-06,\n",
       "           2.1250e-06, 5.4387e-02]]], device='cuda:0'),\n",
       " 'no_vocals')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sd[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from screamdataset import ScreamDataset\n",
    "import torch \n",
    "import torchaudio\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from cnn import CNNNetwork\n",
    "\n",
    "ANNOTATIONS_FILE = '/home/vedant/projects/ScreamDetection/resources/dataset/dataset-pytorch.csv'\n",
    "AUDIO_DIR = '/home/vedant/projects/ScreamDetection/resources/dataset/blocked_audio'\n",
    "BATCH_SIZE = 128\n",
    "SAMPLE_RATE=44100\n",
    "EPOCHS = 30\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "mel_spectrogram = torchaudio.transforms.MelSpectrogram(\n",
    "        sample_rate = 44100,\n",
    "        n_fft = 1024,\n",
    "        hop_length = 512,\n",
    "        n_mels = 64\n",
    "    )\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = 'cuda'\n",
    "else:\n",
    "    DEVICE = 'cpu'\n",
    "\n",
    "def create_data_loader(train_data,batch_size):\n",
    "    train_dataloader = DataLoader(train_data,batch_size=batch_size)\n",
    "    return train_dataloader\n",
    "\n",
    "def train_one_epoch(model, data_loader,loss_function,optimiser,device):\n",
    "    for inputs,targets in data_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        #inputs,targets = inputs.to(device),targets.to(device)\n",
    "\n",
    "        # Calculate Loss\n",
    "        predictions = model(inputs)\n",
    "        loss = loss_function(predictions,targets)\n",
    "\n",
    "        # Backpropagate Loss, update weights\n",
    "        optimiser.zero_grad()\n",
    "        loss.backward() # Apply backpropagation\n",
    "        optimiser.step() # Update weights\n",
    "    print(f\"Loss : {loss.item()}\")\n",
    "\n",
    "def train(model, data_loader,loss_function, optimiser, device, epochs):\n",
    "    for i in range(epochs):\n",
    "        print(f\"Epoch {i+1}:\")\n",
    "        train_one_epoch(model, data_loader, loss_function, optimiser, device)\n",
    "        print(\"-------------------------------------------------------\")\n",
    "    print(\"Training done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Epoch 1:\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1390230/3946499839.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m#Train Model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimiser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEVICE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m#Save results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1390230/1148303004.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, data_loader, loss_function, optimiser, device, epochs)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {i+1}:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimiser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-------------------------------------------------------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training done\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1390230/1148303004.py\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, data_loader, loss_function, optimiser, device)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtargets\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0;31m#inputs,targets = inputs.to(device),targets.to(device)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    DEVICE = 'cuda'\n",
    "else:\n",
    "    DEVICE = 'cpu'\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "\n",
    "\n",
    "#instantiating dataset object and transform\n",
    "mel_spectrogram = torchaudio.transforms.MelSpectrogram(\n",
    "    sample_rate = SAMPLE_RATE,\n",
    "    n_fft = 1024,\n",
    "    hop_length = 512,\n",
    "    n_mels = 64\n",
    ")\n",
    "\n",
    "sd = ScreamDataset(ANNOTATIONS_FILE, AUDIO_DIR, mel_spectrogram, DEVICE)\n",
    "train_dataloader = create_data_loader(sd,BATCH_SIZE)\n",
    "\n",
    "cnn = CNNNetwork().to(DEVICE)\n",
    "# Instantiating loss function and optimiser\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimiser=torch.optim.Adam(cnn.parameters(),\n",
    "                        lr=LEARNING_RATE\n",
    "                            )\n",
    "\n",
    "#Train Model\n",
    "train(cnn,train_dataloader, loss_function, optimiser, DEVICE, EPOCHS)\n",
    "\n",
    "#Save results\n",
    "torch.save(cnn.state_dict(),\"/home/vedant/projects/ScreamDetection/CNN/trained_models/scream_cnn.pth\")\n",
    "\n",
    "print(\"Model trained and stored at /CNN/trained_models/scream_cnn.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('no_vocals', 'no_vocals', 'no_vocals', 'no_vocals', 'no_vocals', 'no_vocals', 'no_vocals', 'no_vocals', 'no_vocals', 'no_vocals', 'no_vocals', 'no_vocals', 'no_vocals', 'no_vocals', 'no_vocals', 'no_vocals', 'no_vocals', 'no_vocals', 'no_vocals', 'no_vocals', 'no_vocals', 'no_vocals', 'no_vocals', 'no_vocals', 'no_vocals', 'no_vocals', 'no_vocals', 'no_vocals', 'no_vocals', 'no_vocals', 'no_vocals', 'no_vocals', 'no_vocals', 'no_vocals', 'no_vocals', 'no_vocals', 'no_vocals', 'no_vocals', 'no_vocals', 'no_vocals', 'no_vocals', 'no_vocals', 'no_vocals', 'no_vocals', 'no_vocals', 'no_vocals', 'no_vocals', 'no_vocals', 'no_vocals', 'no_vocals', 'no_vocals', 'no_vocals', 'no_vocals', 'no_vocals', 'no_vocals', 'no_vocals', 'no_vocals', 'no_vocals', 'no_vocals', 'no_vocals', 'no_vocals', 'no_vocals', 'no_vocals', 'no_vocals', 'no_vocals', 'no_vocals', 'no_vocals', 'no_vocals', 'no_vocals', 'no_vocals', 'no_vocals', 'midfry', 'midfry', 'midfry', 'midfry', 'midfry', 'midfry', 'midfry', 'midfry', 'midfry', 'midfry', 'midfry', 'midfry', 'midfry', 'midfry', 'midfry', 'midfry', 'midfry', 'midfry', 'midfry', 'no_vocals', 'midfry', 'midfry', 'midfry', 'midfry', 'midfry', 'midfry', 'no_vocals', 'midfry', 'midfry', 'midfry', 'midfry', 'midfry', 'midfry', 'midfry', 'midfry', 'midfry', 'midfry', 'midfry', 'midfry', 'midfry', 'midfry', 'clean', 'clean', 'clean', 'clean', 'clean', 'clean', 'clean', 'clean', 'clean', 'clean', 'clean', 'clean', 'clean', 'clean', 'midfry', 'midfry')\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "for inputs,targets in train_dataloader:\n",
    "    if i==0:\n",
    "        print(targets)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
